{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd03dbf2-7e12-4b6d-b325-2c4f10ea2ab3",
   "metadata": {},
   "source": [
    "# RAG Using Different LLM from RAG Essentials\n",
    "\n",
    "In this notebook, we demonstrate the construction of a **Retrieval-Augmented Generation (RAG)** pipeline by connecting with **HPE's RAG Essentials**.\n",
    "\n",
    "Our goal is to integrate RAG Essentials into our RAG architecture, where a language model is enhanced with external knowledge retrieval. This allows the system to answer queries based not only on its internal knowledge but also by dynamically retrieving relevant context from an external data source.\n",
    "\n",
    "This notebook covers:\n",
    "- Preparing and indexing documents for retrieval\n",
    "- Setting up RAG Essentials for LLM inference\n",
    "- Querying the RAG system with natural language inputs\n",
    "- Using the retrieved context to generate enriched, accurate answers\n",
    "\n",
    "This example serves as a practical guide to leveraging cloud-based inferencing with HPE RAG Essentials to build scalable and intelligent applications using retrieval-augmented techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc09dd-68b0-4e80-bd03-1de67a5ba542",
   "metadata": {},
   "source": [
    "## Installing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192aa8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1989c",
   "metadata": {},
   "source": [
    "### One-Time Environment Setup (Cert & Dependencies)\n",
    "\n",
    "The following commands should be run **once** at the beginning of your session:\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt -qq > /dev/null 2>&1\n",
    "!cat my-private-ca-pcai-1.crt >> $(python -m certifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016337bc",
   "metadata": {},
   "source": [
    "### Append Custom CA Certificate to Python's Trusted Cert Store\n",
    "\n",
    "The following command appends a custom certificate (`my-private-ca-pcai-1.crt`) to Python's certifi CA bundle, allowing Python tools like `requests` to trust internal HTTPS endpoints signed by this CA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b17097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /mnt/shared/CA/my-private-ca-pcai-1.crt >> $(python -m certifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb03218",
   "metadata": {},
   "source": [
    "### Restart the Kernel\n",
    "\n",
    "After running the setup commands above:\n",
    "\n",
    "Go to the Jupyter menu and select:  \n",
    "**`Kernel` → `Restart Kernel`**  \n",
    "\n",
    "This step is necessary to activate:\n",
    "- The newly installed packages.\n",
    "- The updated CA certificates in the Python runtime.\n",
    "\n",
    "\n",
    "### After Restart: Run the Remaining Notebook Cells\n",
    "\n",
    "Now that the kernel has restarted, begin executing the remaining notebook cells starting from your LangChain and Weaviate imports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc737e3-dc25-4744-9295-7fcd26c2de29",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "This cell imports all necessary libraries and modules required to build the RAG pipeline. It includes:\n",
    "\n",
    "- **LangChain integrations** for:\n",
    "  - NVIDIA AI Endpoints (chat and reranking)\n",
    "  - Weaviate vector store\n",
    "  - Document loaders and text splitters\n",
    "  - RetrievalQA chains with contextual compression\n",
    "- **Weaviate** for managing the vector database\n",
    "\n",
    "These components collectively enable document ingestion, vector storage, retrieval, reranking, and response generation using cloud-hosted LLM endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0197765d-6014-4434-bb59-daf1ea4534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import copy\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d529650-d181-4dff-a9b5-e2bc2ef62bad",
   "metadata": {},
   "source": [
    "##  Fetching the Secret Token for RAG Essentials\n",
    "\n",
    "This step retrieves the **secret access token** required to authenticate and connect to the **Weaviate vector database** instance used in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a304d1-baba-4586-bd23-bddf732c3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "#getting the auth token\n",
    "secret_file_path = \"/etc/secrets/ezua/.auth_token\"\n",
    "\n",
    "with open(secret_file_path, \"r\") as file:\n",
    "    token = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb97a8d-2021-4ef5-86fd-66f611bdedf1",
   "metadata": {},
   "source": [
    "##  Connecting to Weaviate\n",
    "\n",
    "This cell establishes a connection to the **Weaviate vector database** using custom HTTP and gRPC endpoints configured for an internal HPE environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263a85b7-022f-43f6-a730-42f73e969422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/meta \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "domain = \".cluster.local\"\n",
    "http_host = \"weaviate.hpe-weaviate.svc.cluster.local\"\n",
    "grpc_host = \"weaviate-grpc.hpe-weaviate.svc\" + domain\n",
    "weaviate_headers = {\"x-auth-token\": token}\n",
    "#weaviate_headers = {\"x-auth-token\": \"wrong token\"}\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=http_host,        # Hostname for the HTTP API connection\n",
    "    http_port=80,              # Default is 80, WCD uses 443\n",
    "    http_secure=False,           # Whether to use https (secure) for the HTTP API connection\n",
    "    grpc_host=grpc_host,        # Hostname for the gRPC API connection\n",
    "    grpc_port=50051,              # Default is 50051, WCD uses 443\n",
    "    grpc_secure=False,           # Whether to use a secure channel for the gRPC API connection\n",
    "    headers=weaviate_headers,\n",
    "    skip_init_checks=False\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74898d-a73f-4ede-831c-6832fb995102",
   "metadata": {},
   "source": [
    "## Connecting to LLM Through RAG ESSENTIALS\n",
    "\n",
    "This cell initializes a connection to a **Large Language Model (LLM)** served through **HPE's RAG Essentials**.\n",
    "\n",
    "- **Model**: `meta/llama3.1-8b-instruct` – a powerful instruction-tuned LLM.\n",
    "- **Endpoint**: Provided by the RAG Essentials `base_url`.\n",
    "- **API Key**: Used for secure access to the inference service.\n",
    "- **Parameters**:\n",
    "  - `temperature`: Controls randomness in outputs (0.5 = balanced)\n",
    "  - `max_tokens`: Limits response length\n",
    "  - `top_p`: Controls nucleus sampling (1.0 = full probability mass)\n",
    "\n",
    "This LLM is later used in the RAG pipeline for generating responses based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2754a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_rag_essentials = \"paste the rag essentials endpoint here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753ad95-48af-4376-81e5-d5d0ec855a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(\n",
    "    base_url=llm_endpoint_rag_essentials,\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    api_key=token,\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1.0,\n",
    ")\n",
    "llm.invoke(\"What cloud services does HPE provide ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d1851-c89b-4a96-908d-73978a46498a",
   "metadata": {},
   "source": [
    "## Data Extraction and Processing\n",
    "\n",
    "This section handles the **loading and preprocessing of PDF documents** used in the RAG pipeline.\n",
    "\n",
    "### Steps:\n",
    "1. **Directory Loading**:\n",
    "   - Loads all PDF files from the `./pdf` directory using `PyPDFDirectoryLoader`.\n",
    "\n",
    "2. **Text Chunking**:\n",
    "   - Splits documents into manageable chunks using `RecursiveCharacterTextSplitter`.\n",
    "   - Parameters: \n",
    "     - `chunk_size=500` characters\n",
    "     - `chunk_overlap=50` characters for better context continuity.\n",
    "\n",
    "3. **Metadata Normalization**:\n",
    "   - Extracts and standardizes metadata fields (e.g., `source`, `page`, `total_pages`, `title`) for each chunk.\n",
    "   - This metadata is crucial for **citations during inference**, helping ensure **traceability and credibility** in generated responses.\n",
    "\n",
    "This prepares the document data for indexing into the vector store with relevant context and citation support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74087bad-8c82-4c54-bbd3-033e77ffaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the PDF is stored\n",
    "pdf_directory = \"./pdf\"\n",
    "# Load the PDF documents\n",
    "loader = PyPDFDirectoryLoader(pdf_directory)\n",
    "documents = loader.load()\n",
    " \n",
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "metadata_docs = [copy.deepcopy(doc) for doc in docs]\n",
    "for doc in docs:\n",
    "    temp_meta = {\"source\": doc.metadata['source'] if not hasattr(doc.metadata,'source') else \"\",\n",
    "                \"page\": float(doc.metadata[\"page_label\"]) if not hasattr(doc.metadata,'page_label') else 0,\n",
    "                \"total_pages\": float(doc.metadata[\"total_pages\"]) if not hasattr(doc.metadata,'total_pages') else 0,\n",
    "                \"title\": doc.metadata[\"title\"] if hasattr(doc.metadata,'title') else \"\"}\n",
    "    doc.metadata = temp_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8b83e-3e1b-48c2-99e3-1ba7d4fab411",
   "metadata": {},
   "source": [
    "##  Vector Store Initialization\n",
    "\n",
    "This section initializes the **vector store** by creating embeddings from the processed document chunks and storing them in **Weaviate**.\n",
    "\n",
    "### Key Components:\n",
    "- **Embeddings**:\n",
    "  - Generated using the `nomic-embed-text:latest` model via **Ollama**, accessed through LangChain’s `OllamaEmbeddings`.\n",
    "\n",
    "- **Vector Store**:\n",
    "  - Uses `WeaviateVectorStore.from_documents()` to create vector representations of the document chunks.\n",
    "  - Connects to the existing Weaviate client and stores the data under the index name **`RAG`**.\n",
    "\n",
    "Once complete, all embedded chunks with associated metadata are indexed in Weaviate under the **RAG** collection, making them searchable during the retrieval phase of the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22599776-f7c7-47fc-84f9-8946f8da9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/RAG2 \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: POST http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema/RAG2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://ollama.pcai1.genai1.hou/api/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/schema \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://weaviate.hpe-weaviate.svc.cluster.local/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fdc4b370440>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "vector = WeaviateVectorStore.from_documents(docs, embedding=OllamaEmbeddings(model = \"nomic-embed-text:latest\", base_url=\"https://ollama.pcai1.genai1.hou\"), client=client, index_name=\"RAG\", text_key=\"Rag\".lower() + \"_key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eedbd2-d552-473f-b4b3-69adb44f38c2",
   "metadata": {},
   "source": [
    "##  Retriever Initialization\n",
    "\n",
    "This step configures the **Weaviate vector store** as a retriever to enable efficient document retrieval within the RAG pipeline.\n",
    "\n",
    "By calling `vector.as_retriever()`, the vector database is wrapped with retrieval capabilities, allowing the system to fetch the most relevant document chunks based on query embeddings during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73394d2-b53b-414c-b0dc-a0d86327a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b221ae9-07f6-462c-a916-e83d01b511a8",
   "metadata": {},
   "source": [
    "## Reranking for Improved Retrieval\n",
    "\n",
    "This section enhances the retrieval accuracy using a **contextual reranking mechanism**.\n",
    "\n",
    "### Components:\n",
    "- **NVIDIARerank**:\n",
    "  - Uses the `nvidia/nv-rerankqa-mistral-4b-v3` model.\n",
    "  - Re-evaluates and reranks the initially retrieved documents from the vector store based on their relevance to the query.\n",
    "\n",
    "- **ContextualCompressionRetriever**:\n",
    "  - Wraps the original retriever with the reranker.\n",
    "  - Filters and returns only the most contextually relevant chunks, improving the final output quality from the LLM.\n",
    "\n",
    "This reranking step ensures that only the most meaningful documents are passed to the LLM, increasing the **accuracy**, **precision**, and **credibility** of the generated answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa2d767-9425-461c-9b79-27af7ad4bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:154: UserWarning: https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou does not end in /v1, you may have inference and listing issues. This check will be deprecated in the next release. Please ensure /v1 is appended to the provided URL\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compressor = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                          base_url=\"https://reranker-5c3f14b5-predictor-ezai-services.pcai1.genai1.hou\",\n",
    "                          api_key=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f66049a-8cba-4b4a-a09d-cdbdf7e120e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786bc9-a8a3-49c9-b57a-effdd202942b",
   "metadata": {},
   "source": [
    "## User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a345a03-46b7-4c5d-8dba-86d8ae0f0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the key message for the SynergyHub campaign?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c476-1e68-4797-a428-62a3893e1c4d",
   "metadata": {},
   "source": [
    "## Output\n",
    "This cell runs the **RetrievalQA chain**, which:\n",
    "\n",
    "- Uses the **retriever** to fetch relevant document chunks from the vector store based on the user query.\n",
    "- Passes the retrieved context to the **LLM** (`llm`) for generating a meaningful and context-aware response.\n",
    "- Returns the generated answer along with the **source documents** used for citation, ensuring transparency and credibility in the output.\n",
    "\n",
    "The call `chain.invoke(query)` triggers the entire RAG process for the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6a4fc-832c-4e02-91aa-a174d8212e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "resp = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588da35a-d43c-4196-ad67-871bdd0d3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = resp[\"result\"]\n",
    "print(\"Assistant:\", result)\n",
    "for metadata in resp[\"source_documents\"]:\n",
    "    print(f\"Source: {metadata.metadata['source']} Title: {metadata.metadata['title']} Page No: {metadata.metadata['page']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
